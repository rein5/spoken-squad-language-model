{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35463607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from huggingface_hub import Repository, get_full_repo_name, notebook_login\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06779b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SpokenSQuAD data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0490f73ef8d46d1926ef04088967052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9db714913a94c04abac9b74b3f3e6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d541c081b74900a683f57bb22f391d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER44 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b44a87197a347daa8e2d7ffa2cadaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER54 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SpokenSQuAD dataset files\n",
    "spoken_train = 'spoken_train-v1.1.json'\n",
    "spoken_test = 'spoken_test-v1.1.json'\n",
    "spoken_test_WER44 = 'spoken_test-v1.1_WER44.json'\n",
    "spoken_test_WER54 = 'spoken_test-v1.1_WER54.json'\n",
    "\n",
    "# function to re-format json data\n",
    "def reformat_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    examples = []\n",
    "    # iterate over 'data' list\n",
    "    for elem in json_data['data']:\n",
    "        title = elem['title']\n",
    "\n",
    "        # iterate over paragraphs\n",
    "        for paragraph in elem['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "\n",
    "            # iterate over question-answers for this paragraph\n",
    "            for qa in paragraph['qas']:\n",
    "                example = {}\n",
    "                example['id'] = qa['id']\n",
    "                example['title'] = title.strip()\n",
    "                example['context'] = context.strip()\n",
    "                example['question'] = qa['question'].strip()\n",
    "                example['answers'] = {}\n",
    "                example['answers']['answer_start'] = [answer[\"answer_start\"] for answer in qa['answers']]\n",
    "                example['answers']['text'] = [answer[\"text\"] for answer in qa['answers']]\n",
    "                examples.append(example)\n",
    "    \n",
    "    out_dict = {'data': examples}\n",
    "\n",
    "    output_json_file = 'out_'+json_file\n",
    "    with open(output_json_file, 'w') as f:\n",
    "        json.dump(out_dict, f)\n",
    "\n",
    "    return output_json_file\n",
    "\n",
    "\n",
    "print(\"Loading SpokenSQuAD data...\")\n",
    "\n",
    "# reformat json data\n",
    "spoken_train = reformat_json(spoken_train)\n",
    "spoken_test = reformat_json(spoken_test)\n",
    "spoken_test_WER44 = reformat_json(spoken_test_WER44)\n",
    "spoken_test_WER54 = reformat_json(spoken_test_WER54)\n",
    "\n",
    "spoken_squad_dataset = load_dataset('json',\n",
    "                                    data_files= { 'train': spoken_train,\n",
    "                                                  'validation': spoken_test,         # NO NOISE: 22.73% WER\n",
    "                                                  'test_WER44': spoken_test_WER44,   # NOISE V1: 44.22% WER\n",
    "                                                  'test_WER54': spoken_test_WER54 }, # NOISE V2: 54.82% WER\n",
    "                                    field = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating Model and Tokenizer...\n",
      "Model: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture: BERT (base, uncased) + Linear head (2 output logits):\n",
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Model fine-tuned from bert-base-uncased\n",
    "model_checkpoint = \"rein5/bert-base-uncased-finetuned-spoken-squad\"\n",
    "\n",
    "# Uncomment this to use bert-base-uncased instead (e.g. to fine-tune from scratch)\n",
    "#model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "print(\"Instantiating Model and Tokenizer...\")\n",
    "print(\"Model: \" + model_checkpoint)\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(\"\\nModel Architecture: BERT (base, uncased) + Linear head (2 output logits):\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66c03dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246109dddb4f41ca938a5f448bfc3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 384 \n",
    "stride = 64\n",
    "\n",
    "\"\"\" \n",
    "    Function to preprocess the training examples. It performs the following operations:  \n",
    "        - tokenize examples into question-context token sequences of the form: \n",
    "            [CLS] question [SEP] context [SEP]         \n",
    "        - apply windowing with given stride\n",
    "        - compute output labels (start_index, end_index)\n",
    "            - if answer not fully within windowed context, set label to (0, 0)\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # find start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: \n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if answer not fully inside context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "\n",
    "train_dataset = spoken_squad_dataset['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580329f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test data (NO NOISE: 22.73% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d018d33b08e49ceae1dddd1401adfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V1 noise test data (44.22% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3fa943e0cf4ef8a29f8c612d946aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V2 noise test data (54.82% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc9b2632374ced971fec19aedd3cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 5423\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# function to preprocess validation/test examples (performs tokenization, windowing)\n",
    "def process_validation_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offsets = inputs['offset_mapping'][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            offset if sequence_ids[k] == 1 else None for k, offset in enumerate(offsets)\n",
    "        ]\n",
    "\n",
    "    inputs['example_id'] = example_ids\n",
    "    return inputs\n",
    "\n",
    "\n",
    "print(\"Preprocessing test data (NO NOISE: 22.73% WER)...\")\n",
    "validation_dataset = spoken_squad_dataset['validation'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing V1 noise test data (44.22% WER)...\")\n",
    "test_WER44_dataset = spoken_squad_dataset['test_WER44'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER44'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing V2 noise test data (54.82% WER)...\")\n",
    "test_WER54_dataset = spoken_squad_dataset['test_WER54'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER54'].column_names\n",
    ")\n",
    "\n",
    "\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a505c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features): \n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        # loop thru all features associated with example ID\n",
    "        for feature_index in example_to_features[example_id]: \n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "            \n",
    "            start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes: \n",
    "                for end_index in end_indexes: \n",
    "                    # skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None: \n",
    "                        continue\n",
    "                    # skip answers with a length that is either <0 or >max_answer_length\n",
    "                    if end_index < start_index or end_index-start_index+1 > max_answer_length: \n",
    "                        continue\n",
    "                    \n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "        # select answer with best score\n",
    "        if len(answers) > 0: \n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else: \n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "        \n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3915a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train dataloader...\n",
      "Creating validation dataloader...\n",
      "Creating test V1 dataloader...\n",
      "Creating test V2 dataloader...\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "test_WER44_set = test_WER44_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER44_set.set_format(\"torch\")\n",
    "test_WER54_set = test_WER54_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER54_set.set_format(\"torch\")\n",
    "\n",
    "print(\"Creating train dataloader...\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle = True, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=20\n",
    ")\n",
    "\n",
    "print(\"Creating validation dataloader...\")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=20\n",
    ")\n",
    "print(\"Creating test V1 dataloader...\")\n",
    "test_WER44_dataloader = DataLoader(\n",
    "    test_WER44_set, collate_fn=default_data_collator, batch_size=20\n",
    ")\n",
    "print(\"Creating test V2 dataloader...\")\n",
    "test_WER54_dataloader = DataLoader(\n",
    "    test_WER54_set, collate_fn=default_data_collator, batch_size=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe7717c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47644904b1b46b5b2d950ee91f18a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model checkpoints uploaded to: rein5/bert-base-uncased-finetuned-spoken-squad\n",
      "Cloning model repo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fran\\Desktop\\spoken-squad-language-model\\bert-base-uncased-finetuned-spoken-squad is already a clone of https://huggingface.co/rein5/bert-base-uncased-finetuned-spoken-squad. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"bert-base-uncased-finetuned-spoken-squad\"\n",
    "\n",
    "\"\"\" ### CODE USED TO UPLOAD THE FINETUNED MODEL TO \n",
    "    ###    huggingface.co/rein5/bert-base-uncased-finetuned-spoken-squad\n",
    "    \n",
    "notebook_login()\n",
    "\n",
    "model_name = output_dir\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "print(\"Trained model checkpoints uploaded to: \" + repo_name)\n",
    "\n",
    "print(\"Cloning model repo...\")\n",
    "repo = Repository(output_dir, clone_from=repo_name)\n",
    "\n",
    "upload_to_hub = True\n",
    "\"\"\"\n",
    "upload_to_hub = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b290a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "Evaluating model before fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea04906d30414e25b756a88155772109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226856a01ac6483aa809aac3845750b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial metrics: {'exact_match': 0.16819286114744908, 'f1': 6.221580477399483}\n",
      "Epoch 1/2...\n",
      "Step 100/1866, Average Loss: 3.2164\n",
      "Step 200/1866, Average Loss: 2.0450\n",
      "Step 300/1866, Average Loss: 1.8930\n",
      "Step 400/1866, Average Loss: 1.8262\n",
      "Step 500/1866, Average Loss: 1.7346\n",
      "Step 600/1866, Average Loss: 1.6516\n",
      "Step 700/1866, Average Loss: 1.6209\n",
      "Step 800/1866, Average Loss: 1.5943\n",
      "Step 900/1866, Average Loss: 1.5286\n",
      "Step 1000/1866, Average Loss: 1.5187\n",
      "Step 1100/1866, Average Loss: 1.5113\n",
      "Step 1200/1866, Average Loss: 1.4381\n",
      "Step 1300/1866, Average Loss: 1.4730\n",
      "Step 1400/1866, Average Loss: 1.5093\n",
      "Step 1500/1866, Average Loss: 1.4255\n",
      "Step 1600/1866, Average Loss: 1.3903\n",
      "Step 1700/1866, Average Loss: 1.3534\n",
      "Step 1800/1866, Average Loss: 1.3712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a75088a60b46e69afb893d363ffbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc173df77bd4ea38bf18100583e4ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - metrics: {'exact_match': 63.16576340870865, 'f1': 73.80180229828255}\n",
      "Epoch 2/2...\n",
      "Step 100/1866, Average Loss: 0.8438\n",
      "Step 200/1866, Average Loss: 0.8188\n",
      "Step 300/1866, Average Loss: 0.8208\n",
      "Step 400/1866, Average Loss: 0.8036\n",
      "Step 500/1866, Average Loss: 0.8217\n",
      "Step 600/1866, Average Loss: 0.8092\n",
      "Step 700/1866, Average Loss: 0.8248\n",
      "Step 800/1866, Average Loss: 0.7925\n",
      "Step 900/1866, Average Loss: 0.8392\n",
      "Step 1000/1866, Average Loss: 0.8520\n",
      "Step 1100/1866, Average Loss: 0.7753\n",
      "Step 1200/1866, Average Loss: 0.7682\n",
      "Step 1300/1866, Average Loss: 0.8117\n",
      "Step 1400/1866, Average Loss: 0.8091\n",
      "Step 1500/1866, Average Loss: 0.8282\n",
      "Step 1600/1866, Average Loss: 0.7594\n",
      "Step 1700/1866, Average Loss: 0.8602\n",
      "Step 1800/1866, Average Loss: 0.8455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370a68ab8f2140d08c3fb3d095d561a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd28a78fd2b345ba816f9e6da09764d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - metrics: {'exact_match': 63.98803961876285, 'f1': 74.1485122086755}\n",
      "Evaluating model on Test Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780d8d3ad726437c84e3e7915ea4c621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed96d1f2e1b84e9dacc832bdfcd59e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on Test V1 Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04526cd7e0fc4151936f7c6a3097ca11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c3bf66b08e410c89f8098edd9c4fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on Test V2 Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084bcc0baf84a0aa309f19ca6358b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001ec1961c024e66b4495d33e6a064e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= RESULTS =============\n",
      "Test Set    (NO NOISE - 22.73% WER) - exact match: 63.98803961876285, F1 score: 74.1485122086755\n",
      "Test V1 Set (V1 NOISE - 44.22% WER) - exact match: 40.34759857970473, F1 score: 55.20098517888367\n",
      "Test V2 Set (V2 NOISE - 54.82% WER) - exact match: 28.518034012334144, F1 score: 42.237515977572336\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# function to evaluate the model on a give dataset \n",
    "def evaluate_model(model, dataloader, dataset, dataset_before_preprocessing, accelerator=None):\n",
    "    if not accelerator: \n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        model, dataloader = accelerator.prepare(\n",
    "            model, dataloader\n",
    "        )\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(dataset)]\n",
    "    end_logits = end_logits[: len(dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, dataset, dataset_before_preprocessing\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "### TRAINING LOOP\n",
    "def train_model(model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, epochs = 2):\n",
    "        training_steps = epochs * len(train_dataloader)\n",
    "        \n",
    "        # Lists to store metrics for plotting\n",
    "        train_losses = []\n",
    "        val_exact_matches = []\n",
    "        val_f1_scores = []\n",
    "\n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "        model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "            model, optimizer, train_dataloader, eval_dataloader\n",
    "        )\n",
    "\n",
    "        # Evaluate before training\n",
    "        print(\"Evaluating model before fine-tuning...\")\n",
    "        metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "        print(\"Initial metrics:\", metrics)\n",
    "        val_exact_matches.append(metrics['exact_match'])\n",
    "        val_f1_scores.append(metrics['f1'])\n",
    "\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=training_steps,\n",
    "        )\n",
    "\n",
    "        print_every = 100  # Print loss every 100 steps\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}...\")\n",
    "            # train for 1 epoch\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if (step + 1) % print_every == 0:\n",
    "                    avg_loss = total_loss / print_every\n",
    "                    print(f\"Step {step + 1}/{len(train_dataloader)}, Average Loss: {avg_loss:.4f}\")\n",
    "                    train_losses.append(avg_loss)\n",
    "                    total_loss = 0\n",
    "                    \n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # evaluate after each epoch \n",
    "            #accelerator.print(\"Evaluation...\")\n",
    "            metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "            val_exact_matches.append(metrics['exact_match'])\n",
    "            val_f1_scores.append(metrics['f1'])\n",
    "            print(f\"Epoch {epoch + 1} - metrics: {metrics}\")\n",
    "\n",
    "            # save and upload \n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if upload_to_hub and accelerator.is_main_process: \n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message = f\"Training in progress. Completed epoch {epoch + 1}\", blocking=False\n",
    "                )\n",
    "\n",
    "### Uncomment to fine-tune the model further\n",
    "#notebook_launcher(train_model, num_processes=1) # change num_processes value for multi-gpu training\n",
    "###\n",
    "\n",
    "### EVALUATE FINETUNED MODEL \n",
    "print(\"Evaluating model on Test Set...\")\n",
    "test_metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'])\n",
    "print(\"Evaluating model on Test V1 Set...\")\n",
    "test_v1_metrics = evaluate_model(model, test_WER44_dataloader, test_WER44_dataset, spoken_squad_dataset['test_WER44'])\n",
    "print(\"Evaluating model on Test V2 Set...\")\n",
    "test_v2_metrics = evaluate_model(model, test_WER54_dataloader, test_WER54_dataset, spoken_squad_dataset['test_WER54'])\n",
    "\n",
    "print(\"============= RESULTS =============\")\n",
    "print(\"Test Set    (NO NOISE - 22.73% WER) - exact match: \" + str(test_metrics['exact_match']) + \", F1 score: \" + str(test_metrics['f1']))\n",
    "print(\"Test V1 Set (V1 NOISE - 44.22% WER) - exact match: \" + str(test_v1_metrics['exact_match']) + \", F1 score: \" + str(test_v1_metrics['f1']))\n",
    "print(\"Test V2 Set (V2 NOISE - 54.82% WER) - exact match: \" + str(test_v2_metrics['exact_match']) + \", F1 score: \" + str(test_v2_metrics['f1']))\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c1af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
